# """
# End-to-End Inference Pipeline

# Runs complete vessel 3D reconstruction pipeline:
# Input Video â†’ Preprocessing â†’ Segmentation â†’ Enhancement â†’ 3DGS-SLAM â†’ Export
# """

# import os
# import sys
# from pathlib import Path
# import torch
# import torch.nn as nn
# import numpy as np
# import cv2
# from tqdm import tqdm
# import argparse
# import yaml
# from typing import Dict, List

# # Add project root
# project_root = Path(__file__).parent
# sys.path.insert(0, str(project_root))

# from data.preprocess import VesselPreprocessor, PreprocessingConfig, get_camera_intrinsics
# from models.segmentation.unet_plusplus import create_segmentation_model
# from models.reconstruction.vessel_3dgs import VesselGaussianSLAM, Camera


# class VesselReconstructionPipeline:
#     """Complete vessel 3D reconstruction pipeline"""
    
#     def __init__(self,
#                  segmentation_checkpoint: str,
#                  enhancement_checkpoint: str = None,
#                  config: Dict = None):
#         """
#         Args:
#             segmentation_checkpoint: Path to segmentation model checkpoint
#             enhancement_checkpoint: Path to enhancement model checkpoint (optional)
#             config: Configuration dict
#         """
#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#         self.config = config or {}
        
#         # Load models
#         print("Loading models...")
#         self.segmentation_model = self._load_segmentation_model(segmentation_checkpoint)
        
#         # Enhancement model (optional)
#         if enhancement_checkpoint:
#             self.enhancement_model = self._load_enhancement_model(enhancement_checkpoint)
#         else:
#             self.enhancement_model = None
#             print("No enhancement model provided, using original images")
        
#         # Preprocessor
#         self.preprocessor = VesselPreprocessor(PreprocessingConfig())
        
#         print(f"Pipeline initialized on {self.device}")
    
#     def _load_segmentation_model(self, checkpoint_path: str):
#         """Load segmentation model"""
#         checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
#         # Create model from config
#         model_config = checkpoint.get('config', {})
#         model = create_segmentation_model(model_config)
        
#         # Load weights
#         model.load_state_dict(checkpoint['model_state_dict'])
#         model = model.to(self.device)
#         model.eval()
        
#         print(f"Loaded segmentation model from {checkpoint_path}")
#         return model
    
#     def _load_enhancement_model(self, checkpoint_path: str):
#         """Load enhancement model (placeholder)"""
#         # TODO: Implement when enhancement model is ready
#         print(f"Enhancement model loading not yet implemented")
#         return None
#     @torch.no_grad()
#     def segment_frame(self, image: np.ndarray) -> Dict[str, np.ndarray]:
#         # 1. è®°å½•åŸå§‹å°ºå¯¸ä»¥ä¾¿åé¢æ¢å¤
#         orig_h, orig_w = image.shape[:2]
        
#         # 2. è®¡ç®—æœ€æ¥è¿‘çš„ 32 çš„å€æ•°
#         new_h = (orig_h // 32) * 32
#         new_w = (orig_w // 32) * 32
        
#         # 3. å¦‚æœå°ºå¯¸ä¸ç¬¦åˆï¼Œè¿›è¡Œç¼©æ”¾
#         if orig_h != new_h or orig_w != new_w:
#             image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        
#         # ... åŸæœ‰çš„é¢„å¤„ç†ä»£ç  (ç°åº¦è½¬æ¢, å½’ä¸€åŒ–) ...
#         if len(image.shape) == 3:
#             image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        
#         image_tensor = torch.from_numpy(image).unsqueeze(0).unsqueeze(0).to(self.device)
#         if image_tensor.max() > 1.0:
#             image_tensor = image_tensor / 255.0

#         # 4. æ¨¡å‹æ¨ç†
#         outputs = self.segmentation_model(image_tensor)
        
#         # 5. è·å–ç»“æœå¹¶è¿˜åŸåˆ°åŸå§‹å°ºå¯¸
#         mask = outputs['mask'].squeeze().cpu().numpy()
#         if mask.shape != (orig_h, orig_w):
#             mask = cv2.resize(mask, (orig_w, orig_h), interpolation=cv2.INTER_LINEAR)
        
#         # éª¨æ¶å’Œç½®ä¿¡åº¦ä¹ŸåŒæ­¥è¿˜åŸ
#         skeleton = outputs.get('skeleton', outputs['mask']).squeeze().cpu().numpy()
#         if skeleton.shape != (orig_h, orig_w):
#             skeleton = cv2.resize(skeleton, (orig_w, orig_h), interpolation=cv2.INTER_LINEAR)
        
#         return {
#             'mask': mask,
#             'skeleton': skeleton,
#             'confidence': np.ones_like(mask) # ç®€åŒ–å¤„ç†
#         }
#     # @torch.no_grad()
#     # def segment_frame(self, image: np.ndarray) -> Dict[str, np.ndarray]:
#     #     """
#     #     Segment vessel in single frame
        
#     #     Args:
#     #         image: Input image (H, W) or (H, W, 3)
            
#     #     Returns:
#     #         Dict with mask, skeleton, confidence
#     #     """
#     #     # Preprocess
#     #     if len(image.shape) == 3:
#     #         image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        
#     #     # Normalize
#     #     image = image.astype(np.float32)
#     #     if image.max() > 1.0:
#     #         image = image / 255.0
        
#     #     # To tensor
#     #     image_tensor = torch.from_numpy(image).unsqueeze(0).unsqueeze(0).to(self.device)
        
#     #     # Segment
#     #     outputs = self.segmentation_model(image_tensor)
        
#     #     # Convert to numpy
#     #     mask = outputs['mask'].squeeze().cpu().numpy()
#     #     skeleton = outputs.get('skeleton', mask).squeeze().cpu().numpy()
#     #     confidence = outputs.get('confidence', np.ones_like(mask)).squeeze().cpu().numpy()
        
#     #     return {
#     #         'mask': mask,
#     #         'skeleton': skeleton,
#     #         'confidence': confidence
#     #     }
    
#     def enhance_frame(self, image: np.ndarray, mask: np.ndarray) -> np.ndarray:
#         """
#         Enhance single frame
        
#         Args:
#             image: Input image (H, W)
#             mask: Vessel mask (H, W)
            
#         Returns:
#             Enhanced image (H, W)
#         """
#         if self.enhancement_model is None:
#             return image
        
#         # TODO: Implement enhancement
#         return image
    
#     def run_pipeline(self,
#                     input_path: str,
#                     output_dir: str,
#                     max_frames: int = None):
#         """
#         Run complete pipeline
        
#         Args:
#             input_path: Path to input video or image sequence
#             output_dir: Output directory
#             max_frames: Maximum frames to process (None for all)
#         """
#         output_path = Path(output_dir)
#         output_path.mkdir(parents=True, exist_ok=True)
        
#         # Create output subdirectories
#         (output_path / 'frames').mkdir(exist_ok=True)
#         (output_path / 'masks').mkdir(exist_ok=True)
#         (output_path / 'enhanced').mkdir(exist_ok=True)
#         (output_path / 'visualization').mkdir(exist_ok=True)
        
#         # Load video
#         print(f"Loading video from {input_path}...")
#         frames = self._load_video(input_path, max_frames)
#         print(f"Loaded {len(frames)} frames")
        
#         # Step 1: Preprocess
#         print("\nStep 1: Preprocessing...")
#         preprocessed = self.preprocessor.process_sequence(frames, select_keyframes=False)
#         processed_frames = preprocessed['processed_frames']
#         keyframe_indices = preprocessed.get('keyframe_indices', list(range(len(frames))))
        
#         print(f"Selected {len(keyframe_indices)} keyframes")
        
#         # Step 2: Segment all frames
#         print("\nStep 2: Segmenting vessels...")
#         segmentation_results = []
        
#         for i, frame in enumerate(tqdm(processed_frames, desc='Segmentation')):
#             result = self.segment_frame(frame)
#             segmentation_results.append(result)
            
#             # Save mask
#             mask_img = (result['mask'] * 255).astype(np.uint8)
#             cv2.imwrite(str(output_path / 'masks' / f'mask_{i:04d}.png'), mask_img)
        
#         # Step 3: Enhance frames
#         print("\nStep 3: Enhancing images...")
#         enhanced_frames = []
        
#         for i, (frame, seg_result) in enumerate(tqdm(
#             zip(processed_frames, segmentation_results),
#             desc='Enhancement',
#             total=len(processed_frames)
#         )):
#             enhanced = self.enhance_frame(frame, seg_result['mask'])
#             enhanced_frames.append(enhanced)
            
#             # Save enhanced
#             enhanced_img = (enhanced * 255).astype(np.uint8)
#             cv2.imwrite(str(output_path / 'enhanced' / f'enhanced_{i:04d}.png'), enhanced_img)
        
#         # Step 4: 3D Reconstruction (only on keyframes)
#         print("\nStep 4: 3D Reconstruction...")
        
#         # Setup camera
#         h, w = processed_frames[0].shape[:2]
#         intrinsics = get_camera_intrinsics((h, w), fov=60.0)
#         camera = Camera(
#             width=w,
#             height=h,
#             fx=intrinsics['fx'],
#             fy=intrinsics['fy'],
#             cx=intrinsics['cx'],
#             cy=intrinsics['cy']
#         )
        
#         # Create SLAM system
#         slam = VesselGaussianSLAM(
#             camera=camera,
#             initial_points=5000,
#             vessel_weight=2.0
#         )
        
#         # Process keyframes
#         for kf_idx in tqdm(keyframe_indices[:min(10, len(keyframe_indices))], desc='SLAM'):
#             # Get frame and mask
#             frame_rgb = np.stack([enhanced_frames[kf_idx]] * 3, axis=0)
#             frame_tensor = torch.from_numpy(frame_rgb).float().to(self.device)
            
#             mask_tensor = torch.from_numpy(segmentation_results[kf_idx]['mask']).unsqueeze(0).to(self.device)
            
#             # Add keyframe
#             slam.add_keyframe(kf_idx, frame_tensor, mask_tensor)
        
#         # Step 5: Export results
#         print("\nStep 5: Exporting results...")
        
#         # Export point cloud
#         point_cloud_path = output_path / 'reconstruction.ply'
#         slam.export_point_cloud(str(point_cloud_path))
        
#         # Save camera trajectory
#         trajectory = []
#         for kf in slam.keyframes:
#             pose = kf['pose_param'].detach().cpu().numpy()
#             trajectory.append(pose)
        
#         trajectory = np.array(trajectory)
#         np.save(output_path / 'trajectory.npy', trajectory)
        
#         # Create visualization video
#         self._create_visualization_video(
#             processed_frames,
#             segmentation_results,
#             enhanced_frames,
#             output_path
#         )
        
#         print(f"\nPipeline completed! Results saved to {output_dir}")
        
#         # Print summary
#         self._print_summary(output_path, len(frames), len(keyframe_indices))
    
#     def _load_video(self, video_path: str, max_frames: int = None) -> List[np.ndarray]:
#         """Load video frames"""
#         video_path = Path(video_path)
#         frames = []
        
#         if video_path.is_file() and video_path.suffix in ['.mp4', '.avi', '.mov']:
#             # Load from video file
#             cap = cv2.VideoCapture(str(video_path))
            
#             frame_idx = 0
#             target_size = None
#             while True:
#                 ret, frame = cap.read()
#                 if not ret:
#                     break
                
#                 frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
#                 if target_size is None:
#                     target_size=(frame.shape[1],frame.shape[0])
#                 else:
#                     if (frame.shape[1],frame.shape[0]) != target_size:
#                         frame=cv2.resize(frame,target_size)
#                 frames.append(frame)
                
#                 frame_idx += 1
#                 if max_frames and frame_idx >= max_frames:
#                     break
            
#             cap.release()
        
#         elif video_path.is_dir():
#             # Load from image directory
#             image_files = sorted(video_path.glob('*.*'))
#             for i, img_path in enumerate(image_files):
#                 if max_frames and i >= max_frames:
#                     break
                
#                 frame = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)
#                 frames.append(frame)
        
#         else:
#             raise ValueError(f"Invalid input path: {video_path}")
        
#         return frames
    
#     def _create_visualization_video(self,
#                                    original_frames: List[np.ndarray],
#                                    segmentation_results: List[Dict],
#                                    enhanced_frames: List[np.ndarray],
#                                    output_path: Path):
#         """Create visualization video"""
#         print("Creating visualization video...")
        
#         h, w = original_frames[0].shape[:2]
        
#         # Create video writer
#         fourcc = cv2.VideoWriter_fourcc(*'mp4v')
#         out = cv2.VideoWriter(
#             str(output_path / 'visualization.mp4'),
#             fourcc,
#             10.0,  # FPS
#             (w * 3, h)
#         )
        
#         for orig, seg, enh in zip(original_frames, segmentation_results, enhanced_frames):
#             # Normalize to 0-255
#             orig_vis = (orig * 255).astype(np.uint8)
#             mask_vis = (seg['mask'] * 255).astype(np.uint8)
#             enh_vis = (enh * 255).astype(np.uint8)
            
#             # Convert to color
#             orig_vis = cv2.cvtColor(orig_vis, cv2.COLOR_GRAY2BGR)
#             mask_vis = cv2.applyColorMap(mask_vis, cv2.COLORMAP_JET)
#             enh_vis = cv2.cvtColor(enh_vis, cv2.COLOR_GRAY2BGR)
            
#             # Concatenate
#             vis = np.hstack([orig_vis, mask_vis, enh_vis])
            
#             out.write(vis)
        
#         out.release()
#         print(f"Visualization saved to {output_path / 'visualization.mp4'}")
    
#     def _print_summary(self, output_path: Path, num_frames: int, num_keyframes: int):
#         """Print pipeline summary"""
#         print("\n" + "="*50)
#         print("PIPELINE SUMMARY")
#         print("="*50)
#         print(f"Total frames processed: {num_frames}")
#         print(f"Keyframes selected: {num_keyframes}")
#         print(f"\nOutputs:")
#         print(f"  - Segmentation masks: {output_path / 'masks'}")
#         print(f"  - Enhanced images: {output_path / 'enhanced'}")
#         print(f"  - 3D point cloud: {output_path / 'reconstruction.ply'}")
#         print(f"  - Camera trajectory: {output_path / 'trajectory.npy'}")
#         print(f"  - Visualization video: {output_path / 'visualization.mp4'}")
#         print("="*50)

# def main():
#     # ================= é…ç½®åŒºåŸŸ (è¯·åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„è·¯å¾„) =================
#     # è¾“å…¥è§†é¢‘è·¯å¾„æˆ–å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
#     input_path = "22.mp4" 
    
#     # è¾“å‡ºç»“æœä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„
#     output_dir = "output_results"
    
#     # è®­ç»ƒå¥½çš„åˆ†å‰²æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ (.pth)
#     seg_checkpoint_path = r"checkpoints\step2_segmentation\best_model.pth"
    
#     # (å¯é€‰) å¢å¼ºæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰å°±è®¾ä¸º None
#     enh_checkpoint_path = None 
    
#     # (å¯é€‰) æœ€å¤§å¤„ç†å¸§æ•°ï¼ŒNone è¡¨ç¤ºå¤„ç†å…¨éƒ¨
#     max_frames_process = None 
#     # =================================================================

#     print(f"æ­£åœ¨è¿è¡Œæ¨ç†...")
#     print(f"è¾“å…¥: {input_path}")
#     print(f"è¾“å‡º: {output_dir}")
#     print(f"æ¨¡å‹: {seg_checkpoint_path}")

#     # åˆ›å»ºè¾“å‡ºç›®å½•
#     os.makedirs(output_dir, exist_ok=True)

#     # åˆ›å»ºæ¨ç†ç®¡çº¿
#     # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„å˜é‡ï¼Œä¸å†ä½¿ç”¨ argparse
#     pipeline = VesselReconstructionPipeline(
#         segmentation_checkpoint=seg_checkpoint_path,
#         enhancement_checkpoint=enh_checkpoint_path,
#         config=None
#     )
    
#     # è¿è¡Œç®¡çº¿
#     pipeline.run_pipeline(
#         input_path=input_path,
#         output_dir=output_dir,
#         max_frames=max_frames_process
#     )

# if __name__ == "__main__":
#     main()

"""
End-to-End Inference Pipeline (ä¼˜åŒ–ç‰ˆ)

Runs complete vessel 3D reconstruction pipeline:
Input Video â†’ Preprocessing â†’ Segmentation â†’ Enhancement â†’ 3DGS-SLAM â†’ Export

ä¼˜åŒ–å†…å®¹ï¼š
- æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
- æ”¹è¿›å°ºå¯¸å¤„ç†ï¼ˆpaddingæ›¿ä»£resizeï¼‰
- æ¨¡å‹è¾“å‡ºéªŒè¯
- å¼‚å¸¸å¤„ç†
- ä¸­é—´ç»“æœä¿å­˜
"""

import os
import sys
from pathlib import Path
import torch
import torch.nn as nn
import numpy as np
import cv2
from tqdm import tqdm
import argparse
import yaml
from typing import Dict, List
import traceback

# Add project root
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from data.preprocess import VesselPreprocessor, PreprocessingConfig, get_camera_intrinsics
from models.segmentation.unet_plusplus import create_segmentation_model
from models.reconstruction.vessel_3dgs import VesselGaussianSLAM, Camera


class VesselReconstructionPipeline:
    """Complete vessel 3D reconstruction pipeline"""
    
    def __init__(self,
                 segmentation_checkpoint: str,
                 enhancement_checkpoint: str = None,
                 config: Dict = None,
                 debug: bool = True):
        """
        Args:
            segmentation_checkpoint: Path to segmentation model checkpoint
            enhancement_checkpoint: Path to enhancement model checkpoint (optional)
            config: Configuration dict
            debug: Enable debug mode
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.config = config or {}
        self.debug = debug
        
        # Load models
        print("=" * 60)
        print("åˆå§‹åŒ–æ¨ç†ç®¡çº¿...")
        print("=" * 60)
        print(f"è®¾å¤‡: {self.device}")
        print(f"è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if debug else 'å…³é—­'}")
        
        self.segmentation_model = self._load_segmentation_model(segmentation_checkpoint)
        
        # Enhancement model (optional)
        if enhancement_checkpoint:
            self.enhancement_model = self._load_enhancement_model(enhancement_checkpoint)
        else:
            self.enhancement_model = None
            print("âš ï¸  æœªæä¾›å¢å¼ºæ¨¡å‹ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
        
        # Preprocessor
        self.preprocessor = VesselPreprocessor(PreprocessingConfig())
        
        print(f"âœ“ ç®¡çº¿åˆå§‹åŒ–å®Œæˆ")
        print("=" * 60)
    
    def _load_segmentation_model(self, checkpoint_path: str):
        """åŠ è½½åˆ†å‰²æ¨¡å‹ï¼ˆå¸¦éªŒè¯ï¼‰"""
        print(f"\næ­£åœ¨åŠ è½½åˆ†å‰²æ¨¡å‹: {checkpoint_path}")
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)  # MB
        print(f"æ¨¡å‹æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # æ‰“å°checkpointä¿¡æ¯
            print(f"Checkpoint åŒ…å«çš„é”®: {list(checkpoint.keys())}")
            
            if 'epoch' in checkpoint:
                print(f"è®­ç»ƒè½®æ•°: {checkpoint['epoch']}")
            if 'best_dice' in checkpoint:
                print(f"æœ€ä½³ Dice: {checkpoint['best_dice']:.4f}")
            if 'best_iou' in checkpoint:
                print(f"æœ€ä½³ IoU: {checkpoint['best_iou']:.4f}")
            
            # Create model from config
            model_config = checkpoint.get('config', {})
            print(f"æ¨¡å‹é…ç½®: {model_config}")
            
            model = create_segmentation_model(model_config)
            
            # Load weights
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            elif 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'])
            else:
                raise KeyError("Checkpointä¸­æ‰¾ä¸åˆ°æ¨¡å‹æƒé‡ (éœ€è¦ 'model_state_dict' æˆ– 'state_dict')")
            
            model = model.to(self.device)
            model.eval()
            
            # éªŒè¯æ¨¡å‹
            self._validate_model(model)
            
            print(f"âœ“ åˆ†å‰²æ¨¡å‹åŠ è½½æˆåŠŸ")
            return model
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            traceback.print_exc()
            raise
    
    def _validate_model(self, model):
        """éªŒè¯æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
        print("éªŒè¯æ¨¡å‹...")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randn(1, 1, 256, 256).to(self.device)
        
        try:
            with torch.no_grad():
                outputs = model(test_input)
            
            print(f"æ¨¡å‹è¾“å‡ºé”®: {outputs.keys()}")
            
            if 'mask' in outputs:
                mask = outputs['mask']
                print(f"Mask shape: {mask.shape}")
                print(f"Mask range: [{mask.min():.3f}, {mask.max():.3f}]")
                
                # æ£€æŸ¥å¼‚å¸¸å€¼
                if torch.isnan(mask).any():
                    print("âš ï¸  è­¦å‘Š: MaskåŒ…å«NaNå€¼")
                if torch.isinf(mask).any():
                    print("âš ï¸  è­¦å‘Š: MaskåŒ…å«Infå€¼")
                
                if mask.min() < -1 or mask.max() > 2:
                    print("âš ï¸  è­¦å‘Š: Maskå€¼èŒƒå›´å¼‚å¸¸ï¼Œåº”è¯¥åœ¨[0, 1]ä¹‹é—´")
            
            print("âœ“ æ¨¡å‹éªŒè¯é€šè¿‡")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {str(e)}")
            raise
    
    def _load_enhancement_model(self, checkpoint_path: str):
        """Load enhancement model (placeholder)"""
        print(f"å¢å¼ºæ¨¡å‹åŠ è½½åŠŸèƒ½å°šæœªå®ç°")
        return None
    
    @torch.no_grad()
    def segment_frame(self, image: np.ndarray, frame_idx: int = -1) -> Dict[str, np.ndarray]:
        """
        åˆ†å‰²å•å¸§å›¾åƒï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            image: è¾“å…¥å›¾åƒ (H, W) or (H, W, 3)
            frame_idx: å¸§ç´¢å¼•ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            
        Returns:
            Dict with mask, skeleton, confidence
        """
        try:
            orig_h, orig_w = image.shape[:2]
            
            if self.debug and frame_idx == 0:
                print(f"\né¦–å¸§åˆ†å‰²è°ƒè¯•ä¿¡æ¯:")
                print(f"åŸå§‹å›¾åƒå°ºå¯¸: {orig_h} x {orig_w}")
            
            # è®¡ç®—paddingï¼ˆä¸ä½¿ç”¨resizeï¼‰
            pad_h = (32 - orig_h % 32) % 32
            pad_w = (32 - orig_w % 32) % 32
            
            # è½¬ç°åº¦
            if len(image.shape) == 3:
                image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # Padding
            if pad_h > 0 or pad_w > 0:
                image_padded = np.pad(image, ((0, pad_h), (0, pad_w)), mode='reflect')
                if self.debug and frame_idx == 0:
                    print(f"Paddingåå°ºå¯¸: {image_padded.shape}")
            else:
                image_padded = image
            
            # å½’ä¸€åŒ–
            image_tensor = torch.from_numpy(image_padded).float().unsqueeze(0).unsqueeze(0).to(self.device)
            if image_tensor.max() > 1.0:
                image_tensor = image_tensor / 255.0
            
            if self.debug and frame_idx == 0:
                print(f"è¾“å…¥tensor - shape: {image_tensor.shape}, range: [{image_tensor.min():.3f}, {image_tensor.max():.3f}]")
            
            # æ¨¡å‹æ¨ç†
            outputs = self.segmentation_model(image_tensor)
            
            # æ£€æŸ¥è¾“å‡º
            if self.debug and frame_idx == 0:
                print(f"æ¨¡å‹è¾“å‡ºé”®: {outputs.keys()}")
                if 'mask' in outputs:
                    print(f"Mask - shape: {outputs['mask'].shape}, range: [{outputs['mask'].min():.3f}, {outputs['mask'].max():.3f}]")
            
            # è·å–maskå¹¶ç§»é™¤padding
            mask = outputs['mask'].squeeze().cpu().numpy()
            mask = mask[:orig_h, :orig_w]
            
            # æ•°å€¼éªŒè¯
            if np.isnan(mask).any() or np.isinf(mask).any():
                print(f"âš ï¸  è­¦å‘Š: å¸§{frame_idx} maskåŒ…å«å¼‚å¸¸å€¼ (NaNæˆ–Inf)")
                mask = np.nan_to_num(mask, nan=0.0, posinf=1.0, neginf=0.0)
            
            # Clipåˆ°åˆç†èŒƒå›´
            mask = np.clip(mask, 0, 1)
            
            if self.debug and frame_idx == 0:
                print(f"æœ€ç»ˆMask - shape: {mask.shape}, range: [{mask.min():.3f}, {mask.max():.3f}], mean: {mask.mean():.3f}")
            
            # è·å–skeleton
            skeleton = outputs.get('skeleton', outputs['mask']).squeeze().cpu().numpy()
            skeleton = skeleton[:orig_h, :orig_w]
            skeleton = np.clip(skeleton, 0, 1)
            
            # è·å–confidenceï¼ˆå¦‚æœæœ‰ï¼‰
            if 'confidence' in outputs:
                confidence = outputs['confidence'].squeeze().cpu().numpy()
                confidence = confidence[:orig_h, :orig_w]
                confidence = np.clip(confidence, 0, 1)
            else:
                confidence = np.ones_like(mask)
            
            return {
                'mask': mask,
                'skeleton': skeleton,
                'confidence': confidence
            }
            
        except Exception as e:
            print(f"âŒ åˆ†å‰²å¸§{frame_idx}å¤±è´¥: {str(e)}")
            traceback.print_exc()
            # è¿”å›ç©ºmask
            return {
                'mask': np.zeros_like(image),
                'skeleton': np.zeros_like(image),
                'confidence': np.zeros_like(image)
            }
    
    def enhance_frame(self, image: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """
        Enhance single frame
        
        Args:
            image: Input image (H, W)
            mask: Vessel mask (H, W)
            
        Returns:
            Enhanced image (H, W)
        """
        if self.enhancement_model is None:
            return image
        
        # TODO: Implement enhancement
        return image
    
    def run_pipeline(self,
                    input_path: str,
                    output_dir: str,
                    max_frames: int = None,
                    save_debug: bool = True):
        """
        è¿è¡Œå®Œæ•´ç®¡çº¿
        
        Args:
            input_path: è¾“å…¥è§†é¢‘æˆ–å›¾åƒåºåˆ—è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            max_frames: æœ€å¤§å¤„ç†å¸§æ•° (Noneè¡¨ç¤ºå…¨éƒ¨)
            save_debug: ä¿å­˜è°ƒè¯•ä¿¡æ¯
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºè¾“å‡ºå­ç›®å½•
        subdirs = ['frames', 'masks', 'enhanced', 'visualization', 'debug']
        for subdir in subdirs:
            (output_path / subdir).mkdir(exist_ok=True)
        
        print("\n" + "=" * 60)
        print("å¼€å§‹å¤„ç†")
        print("=" * 60)
        
        # åŠ è½½è§†é¢‘
        print(f"æ­£åœ¨åŠ è½½è§†é¢‘: {input_path}")
        frames = self._load_video(input_path, max_frames)
        print(f"âœ“ åŠ è½½äº† {len(frames)} å¸§")
        
        if len(frames) == 0:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•å¸§ï¼Œè¯·æ£€æŸ¥è¾“å…¥è·¯å¾„")
            return
        
        # ä¿å­˜ç¬¬ä¸€å¸§ç”¨äºè°ƒè¯•
        if save_debug:
            cv2.imwrite(str(output_path / 'debug' / 'first_frame.png'), frames[0])
        
        # Step 1: é¢„å¤„ç†
        print("\n" + "-" * 60)
        print("æ­¥éª¤ 1: é¢„å¤„ç†")
        print("-" * 60)
        
        try:
            preprocessed = self.preprocessor.process_sequence(frames, select_keyframes=False)
            processed_frames = preprocessed['processed_frames']
            keyframe_indices = preprocessed.get('keyframe_indices', list(range(len(frames))))
            print(f"âœ“ é€‰æ‹©äº† {len(keyframe_indices)} ä¸ªå…³é”®å¸§")
        except Exception as e:
            print(f"âš ï¸  é¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å¸§: {str(e)}")
            processed_frames = frames
            keyframe_indices = list(range(len(frames)))
        
        # Step 2: åˆ†å‰²æ‰€æœ‰å¸§
        print("\n" + "-" * 60)
        print("æ­¥éª¤ 2: è¡€ç®¡åˆ†å‰²")
        print("-" * 60)
        
        segmentation_results = []
        failed_frames = []
        
        for i, frame in enumerate(tqdm(processed_frames, desc='åˆ†å‰²è¿›åº¦')):
            result = self.segment_frame(frame, frame_idx=i)
            segmentation_results.append(result)
            
            # æ£€æŸ¥åˆ†å‰²è´¨é‡
            if result['mask'].max() < 0.01:  # å‡ ä¹å…¨é»‘
                failed_frames.append(i)
            
            # ä¿å­˜mask
            mask_img = (result['mask'] * 255).astype(np.uint8)
            cv2.imwrite(str(output_path / 'masks' / f'mask_{i:04d}.png'), mask_img)
            
            # ä¿å­˜è°ƒè¯•ä¿¡æ¯ï¼ˆå‰5å¸§ï¼‰
            if save_debug and i < 5:
                # ä¿å­˜åŸå§‹å¸§
                cv2.imwrite(str(output_path / 'debug' / f'frame_{i:04d}.png'), 
                           (frame * 255).astype(np.uint8) if frame.max() <= 1.0 else frame)
                
                # ä¿å­˜å åŠ å¯è§†åŒ–
                if len(frame.shape) == 2:
                    frame_vis = cv2.cvtColor((frame * 255).astype(np.uint8) if frame.max() <= 1.0 else frame, 
                                            cv2.COLOR_GRAY2BGR)
                else:
                    frame_vis = frame
                
                mask_colored = cv2.applyColorMap(mask_img, cv2.COLORMAP_JET)
                overlay = cv2.addWeighted(frame_vis, 0.6, mask_colored, 0.4, 0)
                cv2.imwrite(str(output_path / 'debug' / f'overlay_{i:04d}.png'), overlay)
        
        if failed_frames:
            print(f"âš ï¸  è­¦å‘Š: {len(failed_frames)} å¸§åˆ†å‰²ç»“æœå‡ ä¹ä¸ºç©º: {failed_frames[:10]}{'...' if len(failed_frames) > 10 else ''}")
        
        print(f"âœ“ åˆ†å‰²å®Œæˆ")
        
        # Step 3: å¢å¼ºå¸§
        print("\n" + "-" * 60)
        print("æ­¥éª¤ 3: å›¾åƒå¢å¼º")
        print("-" * 60)
        
        enhanced_frames = []
        for i, (frame, seg_result) in enumerate(tqdm(
            zip(processed_frames, segmentation_results),
            desc='å¢å¼ºè¿›åº¦',
            total=len(processed_frames)
        )):
            enhanced = self.enhance_frame(frame, seg_result['mask'])
            enhanced_frames.append(enhanced)
            
            # ä¿å­˜å¢å¼ºç»“æœ
            enhanced_img = (enhanced * 255).astype(np.uint8) if enhanced.max() <= 1.0 else enhanced
            cv2.imwrite(str(output_path / 'enhanced' / f'enhanced_{i:04d}.png'), enhanced_img)
        
        print(f"âœ“ å¢å¼ºå®Œæˆ")
        
        # Step 4: 3Dé‡å»º
        print("\n" + "-" * 60)
        print("æ­¥éª¤ 4: 3D é‡å»º")
        print("-" * 60)
        
        try:
            self._run_3d_reconstruction(
                processed_frames,
                enhanced_frames,
                segmentation_results,
                keyframe_indices,
                output_path
            )
            print(f"âœ“ 3Dé‡å»ºå®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  3Dé‡å»ºå¤±è´¥: {str(e)}")
            traceback.print_exc()
        
        # Step 5: åˆ›å»ºå¯è§†åŒ–è§†é¢‘
        print("\n" + "-" * 60)
        print("æ­¥éª¤ 5: ç”Ÿæˆå¯è§†åŒ–")
        print("-" * 60)
        
        try:
            self._create_visualization_video(
                processed_frames,
                segmentation_results,
                enhanced_frames,
                output_path
            )
            print(f"âœ“ å¯è§†åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  å¯è§†åŒ–å¤±è´¥: {str(e)}")
            traceback.print_exc()
        
        # æ‰“å°æ€»ç»“
        self._print_summary(output_path, len(frames), len(keyframe_indices), failed_frames)
    
    def _run_3d_reconstruction(self,
                              processed_frames,
                              enhanced_frames,
                              segmentation_results,
                              keyframe_indices,
                              output_path):
        """è¿è¡Œ3Dé‡å»º"""
        # Setup camera
        h, w = processed_frames[0].shape[:2]
        intrinsics = get_camera_intrinsics((h, w), fov=60.0)
        camera = Camera(
            width=w,
            height=h,
            fx=intrinsics['fx'],
            fy=intrinsics['fy'],
            cx=intrinsics['cx'],
            cy=intrinsics['cy']
        )
        
        # Create SLAM system
        slam = VesselGaussianSLAM(
            camera=camera,
            initial_points=5000,
            vessel_weight=2.0
        )
        
        # Process keyframes (é™åˆ¶æ•°é‡é¿å…å†…å­˜é—®é¢˜)
        max_keyframes = min(10, len(keyframe_indices))
        for kf_idx in tqdm(keyframe_indices[:max_keyframes], desc='SLAMå¤„ç†'):
            # Get frame and mask
            frame_rgb = np.stack([enhanced_frames[kf_idx]] * 3, axis=0)
            frame_tensor = torch.from_numpy(frame_rgb).float().to(self.device)
            
            mask_tensor = torch.from_numpy(segmentation_results[kf_idx]['mask']).unsqueeze(0).to(self.device)
            
            # Add keyframe
            slam.add_keyframe(kf_idx, frame_tensor, mask_tensor)
        
        # Export point cloud
        point_cloud_path = output_path / 'reconstruction.ply'
        slam.export_point_cloud(str(point_cloud_path))
        
        # Save camera trajectory
        trajectory = []
        for kf in slam.keyframes:
            pose = kf['pose_param'].detach().cpu().numpy()
            trajectory.append(pose)
        
        if trajectory:
            trajectory = np.array(trajectory)
            np.save(output_path / 'trajectory.npy', trajectory)
    
    def _load_video(self, video_path: str, max_frames: int = None) -> List[np.ndarray]:
        """åŠ è½½è§†é¢‘å¸§"""
        video_path = Path(video_path)
        frames = []
        
        if video_path.is_file() and video_path.suffix.lower() in ['.mp4', '.avi', '.mov', '.mkv']:
            # ä»è§†é¢‘æ–‡ä»¶åŠ è½½
            cap = cv2.VideoCapture(str(video_path))
            
            if not cap.isOpened():
                raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            print(f"è§†é¢‘ä¿¡æ¯: {total_frames} å¸§, {fps:.2f} FPS")
            
            frame_idx = 0
            target_size = None
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # è½¬ç°åº¦
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                # ç»Ÿä¸€å°ºå¯¸
                if target_size is None:
                    target_size = (frame.shape[1], frame.shape[0])
                else:
                    if (frame.shape[1], frame.shape[0]) != target_size:
                        frame = cv2.resize(frame, target_size)
                
                frames.append(frame)
                
                frame_idx += 1
                if max_frames and frame_idx >= max_frames:
                    break
            
            cap.release()
        
        elif video_path.is_dir():
            # ä»å›¾åƒç›®å½•åŠ è½½
            image_files = sorted(video_path.glob('*.*'))
            image_files = [f for f in image_files if f.suffix.lower() in ['.png', '.jpg', '.jpeg', '.bmp']]
            
            print(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
            
            for i, img_path in enumerate(image_files):
                if max_frames and i >= max_frames:
                    break
                
                frame = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)
                if frame is not None:
                    frames.append(frame)
        
        else:
            raise ValueError(f"æ— æ•ˆçš„è¾“å…¥è·¯å¾„: {video_path}")
        
        return frames
    
    def _create_visualization_video(self,
                                   original_frames: List[np.ndarray],
                                   segmentation_results: List[Dict],
                                   enhanced_frames: List[np.ndarray],
                                   output_path: Path):
        """åˆ›å»ºå¯è§†åŒ–è§†é¢‘"""
        print("æ­£åœ¨åˆ›å»ºå¯è§†åŒ–è§†é¢‘...")
        
        h, w = original_frames[0].shape[:2]
        
        # Create video writer
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(
            str(output_path / 'visualization.mp4'),
            fourcc,
            10.0,  # FPS
            (w * 3, h)
        )
        
        for i, (orig, seg, enh) in enumerate(zip(original_frames, segmentation_results, enhanced_frames)):
            # å½’ä¸€åŒ–åˆ°0-255
            orig_vis = (orig * 255).astype(np.uint8) if orig.max() <= 1.0 else orig.astype(np.uint8)
            mask_vis = (seg['mask'] * 255).astype(np.uint8)
            enh_vis = (enh * 255).astype(np.uint8) if enh.max() <= 1.0 else enh.astype(np.uint8)
            
            # è½¬å½©è‰²
            orig_vis = cv2.cvtColor(orig_vis, cv2.COLOR_GRAY2BGR)
            mask_vis = cv2.applyColorMap(mask_vis, cv2.COLORMAP_JET)
            enh_vis = cv2.cvtColor(enh_vis, cv2.COLOR_GRAY2BGR)
            
            # æ·»åŠ æ–‡æœ¬æ ‡ç­¾
            cv2.putText(orig_vis, 'Original', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(mask_vis, 'Segmentation', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(enh_vis, 'Enhanced', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            
            # æ·»åŠ å¸§å·
            frame_text = f'Frame: {i}/{len(original_frames)}'
            cv2.putText(orig_vis, frame_text, (10, h - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            
            # æ‹¼æ¥
            vis = np.hstack([orig_vis, mask_vis, enh_vis])
            out.write(vis)
        
        out.release()
        print(f"âœ“ å¯è§†åŒ–è§†é¢‘å·²ä¿å­˜")
    
    def _print_summary(self, output_path: Path, num_frames: int, num_keyframes: int, failed_frames: List[int]):
        """æ‰“å°ç®¡çº¿æ€»ç»“"""
        print("\n" + "=" * 60)
        print("å¤„ç†å®Œæˆæ€»ç»“")
        print("=" * 60)
        print(f"æ€»å¤„ç†å¸§æ•°: {num_frames}")
        print(f"å…³é”®å¸§æ•°é‡: {num_keyframes}")
        
        if failed_frames:
            print(f"âš ï¸  åˆ†å‰²å¤±è´¥å¸§: {len(failed_frames)} å¸§")
        
        print(f"\nè¾“å‡ºæ–‡ä»¶:")
        print(f"  ğŸ“ åˆ†å‰²masks: {output_path / 'masks'}")
        print(f"  ğŸ“ å¢å¼ºå›¾åƒ: {output_path / 'enhanced'}")
        print(f"  ğŸ“ è°ƒè¯•ä¿¡æ¯: {output_path / 'debug'}")
        print(f"  ğŸ¬ å¯è§†åŒ–è§†é¢‘: {output_path / 'visualization.mp4'}")
        
        if (output_path / 'reconstruction.ply').exists():
            print(f"  ğŸ—¿ 3Dç‚¹äº‘: {output_path / 'reconstruction.ply'}")
        if (output_path / 'trajectory.npy').exists():
            print(f"  ğŸ“Š ç›¸æœºè½¨è¿¹: {output_path / 'trajectory.npy'}")
        
        print("=" * 60)
        print("\nğŸ’¡ è°ƒè¯•å»ºè®®:")
        print("1. æ£€æŸ¥ debug æ–‡ä»¶å¤¹ä¸­çš„å‰5å¸§overlayå›¾åƒ")
        print("2. æŸ¥çœ‹ visualization.mp4 ç¡®è®¤åˆ†å‰²æ•ˆæœ")
        print("3. å¦‚æœåˆ†å‰²å…¨é»‘ï¼Œæ£€æŸ¥æ¨¡å‹æƒé‡æ˜¯å¦æ­£ç¡®")
        print("=" * 60)


def main():
    # ================= é…ç½®åŒºåŸŸ =================
    # è¾“å…¥è§†é¢‘è·¯å¾„æˆ–å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
    input_path = "22.mp4"
    
    # è¾“å‡ºç»“æœä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„
    output_dir = "output_results"
    
    # è®­ç»ƒå¥½çš„åˆ†å‰²æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ (.pth)
    seg_checkpoint_path = r"checkpoints\step2_segmentation\best_model.pth"
    
    # (å¯é€‰) å¢å¼ºæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰å°±è®¾ä¸º None
    enh_checkpoint_path = None
    
    # (å¯é€‰) æœ€å¤§å¤„ç†å¸§æ•°ï¼ŒNone è¡¨ç¤ºå¤„ç†å…¨éƒ¨
    max_frames_process = None
    
    # æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆä¼šè¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼‰
    debug_mode = True
    
    # æ˜¯å¦ä¿å­˜è°ƒè¯•æ–‡ä»¶
    save_debug_files = True
    # ===========================================

    print("=" * 60)
    print("è¡€ç®¡3Dé‡å»ºæ¨ç†ç®¡çº¿")
    print("=" * 60)
    print(f"è¾“å…¥: {input_path}")
    print(f"è¾“å‡º: {output_dir}")
    print(f"æ¨¡å‹: {seg_checkpoint_path}")
    print(f"è°ƒè¯•: {'å¼€å¯' if debug_mode else 'å…³é—­'}")
    print("=" * 60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    try:
        # åˆ›å»ºæ¨ç†ç®¡çº¿
        pipeline = VesselReconstructionPipeline(
            segmentation_checkpoint=seg_checkpoint_path,
            enhancement_checkpoint=enh_checkpoint_path,
            config=None,
            debug=debug_mode
        )
        
        # è¿è¡Œç®¡çº¿
        pipeline.run_pipeline(
            input_path=input_path,
            output_dir=output_dir,
            max_frames=max_frames_process,
            save_debug=save_debug_files
        )
        
        print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()