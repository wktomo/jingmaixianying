# import torch
# import torch.nn as nn
# from torch.utils.data import DataLoader
# import hydra
# from omegaconf import DictConfig, OmegaConf
# import wandb
# from tqdm import tqdm
# from pathlib import Path
# import numpy as np

# # å¯¼å…¥ä½ çš„æ¨¡å‹å’ŒæŸå¤±å‡½æ•°ç»„ä»¶
# from data.datasets import VesselEnhancementDataset
# from models.enhancement.vessel_controlnet import VesselDiffusionEnhancer
# from losses.segmentation_losses import VesselWeightedPhotometricLoss

# class EnhancementTrainer:
#     def __init__(self, config: DictConfig):
#         self.config = config
#         self.device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
        
#         # 1. åˆå§‹åŒ–å¢å¼ºæ¨¡å‹ (å†…éƒ¨åŠ è½½ ControlNet å‚æ•°)
#         self.model = VesselDiffusionEnhancer(config).to(self.device)
        
#         # 2. è¡€ç®¡åŠ æƒæŸå¤±å‡½æ•°ï¼šå¯¹è¡€ç®¡åŒºåŸŸæ–½åŠ æ›´é«˜æƒé‡
#         self.criterion = VesselWeightedPhotometricLoss(
#             # vessel_weight=config.loss.vessel_weight, # å»ºè®®è®¾ä¸º 2.0
#             # ssim_weight=0.5
#             vessel_weight=2.0,  
#             ssim_weight=0.5
#         )
        
#         # 3. ä¼˜åŒ–å™¨ï¼šæŒ‰ç…§æ–‡æ¡£å»ºè®®ï¼Œä¼˜å…ˆè®­ç»ƒ ControlNet éƒ¨åˆ†ä»¥ä¿è¯ç¨³å®šæ€§
#         self.optimizer = torch.optim.AdamW(
#             self.model.controlnet.parameters(), 
#             lr=config.training.optimizer.lr
#         )
        
#         # 4. åŠ è½½æ•°æ®é›†ï¼šè¯»å–ä½ ç”Ÿæˆçš„ 3é€šé“ä¸­é—´äº§ç‰©
#         self.train_loader = self._create_dataloader()
        
#         # 5. æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
#         self.scaler = torch.amp.GradScaler('cuda') if config.training.use_amp else None

#     def _create_dataloader(self):
#         # ä½¿ç”¨ä½ ä¹‹å‰å®šä¹‰çš„ Dataset åŠ è½½å™¨
#         my_data_path = r"D:/babba/xxx/jingmaixianying/vessel_3d_recon/data/processed/"
#         dataset = VesselEnhancementDataset(
#             data_root=my_data_path,
#             split='train',
#             image_size=tuple(self.config.data.image_size)
#         )
#         return DataLoader(
#             dataset, 
#             batch_size=self.config.training.batch_size,
#             shuffle=True, 
#             num_workers=4,
#             pin_memory=True
#         )

#     def train_epoch(self, epoch):
#         self.model.train()
#         pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}")
        
#         for batch in pbar:
#             # è·å– 3é€šé“ä¸­é—´äº§ç‰© (condition) å’Œ åŸå§‹å›¾ (image)
#             # low_res = batch['images'].to(self.device)
#             # target = batch['images'].to(self.device)
#             # condition = batch['condition'].to(self.device) # æ©ç +éª¨æ¶+è¾¹ç¼˜
#             # vessel_mask = batch['mask'].to(self.device)

#             low_res = batch['noisy'].to(self.device)   # è¾“å…¥ï¼šå™ªå£°å›¾
#             target = batch['clean'].to(self.device)    # ç›®æ ‡ï¼šé«˜æ¸…å›¾
#             vessel_mask = batch['mask'].to(self.device) # æ©ç ï¼šè®¡ç®— Loss ç”¨
            
#             # ç‰¹åˆ«æ³¨æ„: ä½ çš„ Dataset å¹¶æ²¡æœ‰ç”Ÿæˆä¸“é—¨çš„ 3 é€šé“ conditionã€‚
#             # è¿™é‡Œå¦‚æœæ¨¡å‹éœ€è¦ conditionï¼Œé€šå¸¸ç›´æ¥ä¼ å…¥ maskã€‚
#             # å¦‚æœæ¨¡å‹æŠ¥é”™é€šé“ä¸åŒ¹é… (1 vs 3)ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„â€œç»´åº¦å¤„ç†â€éƒ¨åˆ†ã€‚
#             condition = batch['mask'].to(self.device)
#             with torch.amp.autocast('cuda', enabled=self.scaler is not None):
#                 # æ¨¡å‹æ ¹æ®æ¡ä»¶å¼•å¯¼ç”Ÿæˆå¢å¼ºå›¾åƒ
#                 enhanced = self.model(low_res, condition)
                
#                 # è®¡ç®— Loss
#                 loss = self.criterion(enhanced, target, vessel_mask)
#             # åå‘ä¼ æ’­ä¸æ¢¯åº¦æ›´æ–°
#             self.optimizer.zero_grad(set_to_none=True)
#             if self.scaler:
#                 self.scaler.scale(loss).backward()
#                 self.scaler.step(self.optimizer)
#                 self.scaler.update()
#             else:
#                 loss.backward()
#                 self.optimizer.step()

#             pbar.set_postfix({"loss": f"{loss.item():.4f}"})
            
#             if self.config.logging.use_wandb:
#                 wandb.log({"train/loss": loss.item()})

#     def save_checkpoint(self, epoch):
#         save_dir = Path(self.config.checkpoint.dirpath)
#         save_dir.mkdir(parents=True, exist_ok=True)
#         path = save_dir / f"enhancement_epoch_{epoch}.pth"
#         torch.save({
#             'epoch': epoch,
#             'model_state_dict': self.model.state_dict(),
#             'config': OmegaConf.to_container(self.config)
#         }, path)

# @hydra.main(version_base=None, config_path="configs", config_name="step3_diffusion")
# def main(config: DictConfig):
#     # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
#     if config.logging.use_wandb:
#         wandb.init(project="Vessel-Enhancement-Step3", config=OmegaConf.to_container(config))
    
#     trainer = EnhancementTrainer(config)
    
#     # å¼€å§‹è®­ç»ƒå¾ªç¯
#     for epoch in range(config.training.max_epochs):
#         trainer.train_epoch(epoch)
        
#         # å®šæœŸä¿å­˜æ¨¡å‹
#         if (epoch + 1) % 10 == 0:
#             trainer.save_checkpoint(epoch)

# if __name__ == "__main__":
#     main()
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import hydra
from omegaconf import DictConfig, OmegaConf
import wandb
from tqdm import tqdm
from pathlib import Path
import numpy as np

# å¯¼å…¥ä½ çš„æ¨¡å‹å’ŒæŸå¤±å‡½æ•°ç»„ä»¶
from data.datasets import VesselEnhancementDataset
from models.enhancement.vessel_controlnet import VesselDiffusionEnhancer
from losses.segmentation_losses import VesselWeightedPhotometricLoss

class EnhancementTrainer:
    def __init__(self, config: DictConfig):
        self.config = config
        self.device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
        
        # 1. åˆå§‹åŒ–å¢å¼ºæ¨¡å‹ (å†…éƒ¨åŠ è½½ ControlNet å‚æ•°)
        self.model = VesselDiffusionEnhancer(config).to(self.device)
        
        # 2. è¡€ç®¡åŠ æƒæŸå¤±å‡½æ•°ï¼šå¯¹è¡€ç®¡åŒºåŸŸæ–½åŠ æ›´é«˜æƒé‡
        self.criterion = VesselWeightedPhotometricLoss(
            # vessel_weight=config.loss.vessel_weight, # å»ºè®®è®¾ä¸º 2.0
            # ssim_weight=0.5
            vessel_weight=2.0,  
            ssim_weight=0.5
        )
        
        # 3. ä¼˜åŒ–å™¨ï¼šæŒ‰ç…§æ–‡æ¡£å»ºè®®ï¼Œä¼˜å…ˆè®­ç»ƒ ControlNet éƒ¨åˆ†ä»¥ä¿è¯ç¨³å®šæ€§
        self.optimizer = torch.optim.AdamW(
            self.model.controlnet.parameters(), 
            lr=config.training.optimizer.lr
        )
        
        # 4. åŠ è½½æ•°æ®é›†ï¼šè¯»å–ä½ ç”Ÿæˆçš„ 3é€šé“ä¸­é—´äº§ç‰©
        self.train_loader = self._create_dataloader()
        
        # 5. æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
        self.scaler = torch.amp.GradScaler('cuda') if config.training.use_amp else None

    def _create_dataloader(self):
        # ä½¿ç”¨ä½ ä¹‹å‰å®šä¹‰çš„ Dataset åŠ è½½å™¨
        my_data_path = r"D:/babba/xxx/jingmaixianying/vessel_3d_recon/data/processed/"
        dataset = VesselEnhancementDataset(
            data_root=my_data_path,
            split='train',
            image_size=tuple(self.config.data.image_size)
        )
        return DataLoader(
            dataset, 
            batch_size=self.config.training.batch_size,
            shuffle=True, 
            num_workers=4,
            pin_memory=True
        )

    @staticmethod
    def _to_3channel(img: torch.Tensor) -> torch.Tensor:
        """å°†å•é€šé“å›¾åƒè½¬æ¢ä¸º3é€šé“ï¼ˆä¿®å¤VAEè¾“å…¥è¦æ±‚ï¼‰"""
        if img.shape[1] == 1:
            return img.repeat(1, 3, 1, 1)
        return img

    def train_epoch(self, epoch):
        self.model.train()
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}")
        
        for batch in pbar:
            # è·å– 3é€šé“ä¸­é—´äº§ç‰© (condition) å’Œ åŸå§‹å›¾ (image)
            # low_res = batch['images'].to(self.device)
            # target = batch['images'].to(self.device)
            # condition = batch['condition'].to(self.device) # æ©ç +éª¨æ¶+è¾¹ç¼˜
            # vessel_mask = batch['mask'].to(self.device)

            low_res = batch['noisy'].to(self.device)   # è¾“å…¥ï¼šå™ªå£°å›¾
            target = batch['clean'].to(self.device)    # ç›®æ ‡ï¼šé«˜æ¸…å›¾
            vessel_mask = batch['mask'].to(self.device) # æ©ç ï¼šè®¡ç®— Loss ç”¨
            
            # ç‰¹åˆ«æ³¨æ„: ä½ çš„ Dataset å¹¶æ²¡æœ‰ç”Ÿæˆä¸“é—¨çš„ 3 é€šé“ conditionã€‚
            # è¿™é‡Œå¦‚æœæ¨¡å‹éœ€è¦ conditionï¼Œé€šå¸¸ç›´æ¥ä¼ å…¥ maskã€‚
            # å¦‚æœæ¨¡å‹æŠ¥é”™é€šé“ä¸åŒ¹é… (1 vs 3)ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„â€œç»´åº¦å¤„ç†â€éƒ¨åˆ†ã€‚
            condition = batch['mask'].to(self.device)
            
            # ğŸ”§ ä¿®å¤ï¼šå°†å•é€šé“å›¾åƒè½¬æ¢ä¸º3é€šé“ä»¥æ»¡è¶³VAEè¾“å…¥è¦æ±‚
            low_res = self._to_3channel(low_res)
            target = self._to_3channel(target)
            condition = self._to_3channel(condition)

            with torch.amp.autocast('cuda', enabled=self.scaler is not None):
                # æ¨¡å‹æ ¹æ®æ¡ä»¶å¼•å¯¼ç”Ÿæˆå¢å¼ºå›¾åƒ
                enhanced = self.model(low_res, condition)
                
                # è®¡ç®— Loss
                loss = self.criterion(enhanced, target, vessel_mask)
            # åå‘ä¼ æ’­ä¸æ¢¯åº¦æ›´æ–°
            self.optimizer.zero_grad(set_to_none=True)
            if self.scaler:
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                self.optimizer.step()

            pbar.set_postfix({"loss": f"{loss.item():.4f}"})
            
            if self.config.logging.use_wandb:
                wandb.log({"train/loss": loss.item()})

    def save_checkpoint(self, epoch):
        save_dir = Path(self.config.checkpoint.dirpath)
        save_dir.mkdir(parents=True, exist_ok=True)
        path = save_dir / f"enhancement_epoch_{epoch}.pth"
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'config': OmegaConf.to_container(self.config)
        }, path)

@hydra.main(version_base=None, config_path="configs", config_name="step3_diffusion")
def main(config: DictConfig):
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    if config.logging.use_wandb:
        wandb.init(project="Vessel-Enhancement-Step3", config=OmegaConf.to_container(config))
    
    trainer = EnhancementTrainer(config)
    
    # å¼€å§‹è®­ç»ƒå¾ªç¯
    for epoch in range(config.training.max_epochs):
        trainer.train_epoch(epoch)
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if (epoch + 1) % 10 == 0:
            trainer.save_checkpoint(epoch)

if __name__ == "__main__":
    main()